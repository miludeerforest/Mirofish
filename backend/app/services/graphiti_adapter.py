"""
FalkorDB çŸ¥è¯†å›¾è°±é€‚é…å™¨

åŸºäº FalkorDB åŸç”Ÿåº“å®ç°ï¼Œæä¾›ä¸ ZepEntityReader / GraphBuilderService å…¼å®¹çš„æ¥å£ã€‚
ä½¿ç”¨ OpenAI API è¿›è¡Œå®ä½“å’Œå…³ç³»æå–ã€‚

æ³¨æ„ï¼šç”±äº graphiti-core ä¸ camel-oasis çš„ neo4j ç‰ˆæœ¬å†²çªï¼Œ
æœ¬å®ç°ä½¿ç”¨ FalkorDB åŸç”Ÿåº“ + OpenAI API ç›´æ¥å®ç°å›¾è°±åŠŸèƒ½ã€‚
"""

import asyncio
import uuid
import time
import threading
import json
from typing import Dict, Any, List, Optional, Callable, Set
from dataclasses import dataclass, field
from datetime import datetime

from openai import OpenAI
from falkordb import FalkorDB

from ..config import Config
from ..models.task import TaskManager, TaskStatus
from .text_processor import TextProcessor
from ..utils.logger import get_logger

logger = get_logger('mirofish.falkordb_adapter')


def _sanitize_label(label: str) -> str:
    """
    å°†æ ‡ç­¾åè½¬æ¢ä¸ºæœ‰æ•ˆçš„ Cypher æ ‡è¯†ç¬¦
    FalkorDB/Neo4j çš„æ ‡ç­¾å’Œå…³ç³»ç±»å‹åªæ”¯æŒ ASCII å­—ç¬¦
    """
    import re
    import hashlib
    
    # å¦‚æœæ˜¯çº¯ ASCIIï¼Œç›´æ¥æ¸…ç†å¹¶è¿”å›
    if label.isascii():
        # ç§»é™¤éæ³•å­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯æ•°å­—ä¸‹åˆ’çº¿
        sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', label)
        # ç¡®ä¿ä¸ä»¥æ•°å­—å¼€å¤´
        if sanitized and sanitized[0].isdigit():
            sanitized = 'T_' + sanitized
        return sanitized or 'Entity'
    
    # å¯¹äºä¸­æ–‡ç­‰é ASCII å­—ç¬¦ï¼Œç”Ÿæˆä¸€ä¸ªåŸºäºå“ˆå¸Œçš„çŸ­æ ‡è¯†ç¬¦
    # åŒæ—¶ä¿æŒå¯è¯»æ€§ï¼ˆä½¿ç”¨é¦–å­—æ¯æˆ–æ‹¼éŸ³ç¼©å†™ï¼‰
    hash_suffix = hashlib.md5(label.encode()).hexdigest()[:6]
    return f"Type_{hash_suffix}"


@dataclass
class GraphInfo:
    """å›¾è°±ä¿¡æ¯"""
    graph_id: str
    node_count: int
    edge_count: int
    entity_types: List[str]
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "graph_id": self.graph_id,
            "node_count": self.node_count,
            "edge_count": self.edge_count,
            "entity_types": self.entity_types,
        }


@dataclass
class EntityNode:
    """å®ä½“èŠ‚ç‚¹æ•°æ®ç»“æ„"""
    uuid: str
    name: str
    labels: List[str]
    summary: str
    attributes: Dict[str, Any]
    related_edges: List[Dict[str, Any]] = field(default_factory=list)
    related_nodes: List[Dict[str, Any]] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "uuid": self.uuid,
            "name": self.name,
            "labels": self.labels,
            "summary": self.summary,
            "attributes": self.attributes,
            "related_edges": self.related_edges,
            "related_nodes": self.related_nodes,
        }
    
    def get_entity_type(self) -> Optional[str]:
        """è·å–å®ä½“ç±»å‹ï¼ˆæ’é™¤é»˜è®¤çš„Entityæ ‡ç­¾ï¼‰"""
        for label in self.labels:
            if label not in ["Entity", "Node"]:
                return label
        return None


@dataclass
class FilteredEntities:
    """è¿‡æ»¤åçš„å®ä½“é›†åˆ"""
    entities: List[EntityNode]
    entity_types: Set[str]
    total_count: int
    filtered_count: int
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "entities": [e.to_dict() for e in self.entities],
            "entity_types": list(self.entity_types),
            "total_count": self.total_count,
            "filtered_count": self.filtered_count,
        }


class FalkorDBClient:
    """FalkorDB å®¢æˆ·ç«¯å°è£…"""
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
            
        try:
            self._db = FalkorDB(
                host=Config.FALKORDB_HOST,
                port=Config.FALKORDB_PORT
            )
            self._initialized = True
            logger.info(f"FalkorDB è¿æ¥æˆåŠŸ: {Config.FALKORDB_HOST}:{Config.FALKORDB_PORT}")
        except Exception as e:
            logger.error(f"FalkorDB è¿æ¥å¤±è´¥: {e}")
            raise
    
    def get_graph(self, graph_id: str):
        """è·å–æˆ–åˆ›å»ºå›¾è°±"""
        return self._db.select_graph(graph_id)
    
    def execute_query(self, graph_id: str, query: str, params: Dict = None):
        """æ‰§è¡Œ Cypher æŸ¥è¯¢"""
        graph = self.get_graph(graph_id)
        return graph.query(query, params or {})


def _get_falkordb_client() -> FalkorDBClient:
    """è·å– FalkorDB å®¢æˆ·ç«¯å•ä¾‹"""
    return FalkorDBClient()


def _get_openai_client() -> OpenAI:
    """è·å– OpenAI å®¢æˆ·ç«¯"""
    return OpenAI(
        api_key=Config.LLM_API_KEY,
        base_url=Config.LLM_BASE_URL
    )


class EntityExtractor:
    """ä½¿ç”¨ LLM æå–å®ä½“å’Œå…³ç³»"""
    
    EXTRACTION_PROMPT = """ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»ã€‚

æ–‡æœ¬ï¼š
{text}

æœ¬ä½“å®šä¹‰ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ï¼š
{ontology}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ï¼ŒåŒ…å«ï¼š
1. entities: å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å« name, type, summary, attributes
2. relations: å…³ç³»åˆ—è¡¨ï¼Œæ¯ä¸ªå…³ç³»åŒ…å« source, target, relation_type, fact

ä¸¥æ ¼éµå¾ªä»¥ä¸‹ JSON æ ¼å¼ï¼š
{{
    "entities": [
        {{"name": "å®ä½“åç§°", "type": "å®ä½“ç±»å‹", "summary": "ç®€çŸ­æè¿°", "attributes": {{}}}}
    ],
    "relations": [
        {{"source": "æºå®ä½“å", "target": "ç›®æ ‡å®ä½“å", "relation_type": "å…³ç³»ç±»å‹", "fact": "å…³ç³»çš„äº‹å®æè¿°"}}
    ]
}}

åªè¿”å› JSONï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡æœ¬ã€‚"""

    def __init__(self):
        self.client = _get_openai_client()
        self.model = Config.LLM_MODEL_NAME
    
    def extract(self, text: str, ontology: Dict[str, Any] = None) -> Dict[str, Any]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»"""
        ontology_str = json.dumps(ontology, ensure_ascii=False, indent=2) if ontology else "æ— ç‰¹å®šæœ¬ä½“å®šä¹‰"
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„çŸ¥è¯†å›¾è°±å®ä½“å…³ç³»æå–åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¾“å‡ºã€‚"},
                    {"role": "user", "content": self.EXTRACTION_PROMPT.format(text=text, ontology=ontology_str)}
                ],
                temperature=0.3,
            )
            
            content = response.choices[0].message.content.strip()
            # æå– JSON éƒ¨åˆ†
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0]
            elif "```" in content:
                content = content.split("```")[1].split("```")[0]
            
            result = json.loads(content)
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"JSON è§£æå¤±è´¥: {e}, å†…å®¹: {content[:200]}")
            return {"entities": [], "relations": []}
        except Exception as e:
            logger.error(f"å®ä½“æå–å¤±è´¥: {e}")
            return {"entities": [], "relations": []}


class GraphitiGraphBuilder:
    """
    FalkorDB ç‰ˆå›¾è°±æ„å»ºæœåŠ¡
    æä¾›ä¸ GraphBuilderService å…¼å®¹çš„æ¥å£
    """
    
    def __init__(self, api_key: Optional[str] = None):
        """åˆå§‹åŒ–"""
        self._falkordb = None
        self._extractor = None
        self.task_manager = TaskManager()
        
        # å­˜å‚¨å›¾è°±å…ƒæ•°æ®
        self._graphs: Dict[str, Dict[str, Any]] = {}
        
    @property
    def falkordb(self) -> FalkorDBClient:
        """å»¶è¿Ÿåˆå§‹åŒ– FalkorDB å®¢æˆ·ç«¯"""
        if self._falkordb is None:
            self._falkordb = _get_falkordb_client()
        return self._falkordb
    
    @property
    def extractor(self) -> EntityExtractor:
        """å»¶è¿Ÿåˆå§‹åŒ–å®ä½“æå–å™¨"""
        if self._extractor is None:
            self._extractor = EntityExtractor()
        return self._extractor
    
    def build_graph_async(
        self,
        text: str,
        ontology: Dict[str, Any],
        graph_name: str = "MiroFish Graph",
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        batch_size: int = 3
    ) -> str:
        """
        å¼‚æ­¥æ„å»ºå›¾è°±
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            ontology: æœ¬ä½“å®šä¹‰
            graph_name: å›¾è°±åç§°
            chunk_size: æ–‡æœ¬å—å¤§å°
            chunk_overlap: å—é‡å å¤§å°
            batch_size: æ¯æ‰¹å‘é€çš„å—æ•°é‡
            
        Returns:
            ä»»åŠ¡ID
        """
        # åˆ›å»ºä»»åŠ¡
        task_id = self.task_manager.create_task(
            task_type="graph_build",
            metadata={
                "graph_name": graph_name,
                "chunk_size": chunk_size,
                "text_length": len(text),
                "backend": "falkordb",
            }
        )
        
        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œæ„å»º
        thread = threading.Thread(
            target=self._build_graph_worker,
            args=(task_id, text, ontology, graph_name, chunk_size, chunk_overlap, batch_size)
        )
        thread.daemon = True
        thread.start()
        
        return task_id
    
    def _build_graph_worker(
        self,
        task_id: str,
        text: str,
        ontology: Dict[str, Any],
        graph_name: str,
        chunk_size: int,
        chunk_overlap: int,
        batch_size: int
    ):
        """å›¾è°±æ„å»ºå·¥ä½œçº¿ç¨‹"""
        try:
            self.task_manager.update_task(
                task_id,
                status=TaskStatus.PROCESSING,
                progress=5,
                message="å¼€å§‹æ„å»º FalkorDB å›¾è°±..."
            )
            
            # 1. åˆ›å»ºå›¾è°±
            graph_id = self.create_graph(graph_name)
            self.task_manager.update_task(
                task_id,
                progress=10,
                message=f"å›¾è°±å·²åˆ›å»º: {graph_id}"
            )
            
            # 2. è®¾ç½®æœ¬ä½“
            self.set_ontology(graph_id, ontology)
            self.task_manager.update_task(
                task_id,
                progress=15,
                message="æœ¬ä½“å·²è®¾ç½®"
            )
            
            # 3. æ–‡æœ¬åˆ†å—
            chunks = TextProcessor.split_text(text, chunk_size, chunk_overlap)
            total_chunks = len(chunks)
            self.task_manager.update_task(
                task_id,
                progress=20,
                message=f"æ–‡æœ¬å·²åˆ†å‰²ä¸º {total_chunks} ä¸ªå—"
            )
            
            # 4. åˆ†æ‰¹æ·»åŠ æ•°æ®
            self.add_text_batches(
                graph_id, chunks, ontology, batch_size,
                lambda msg, prog: self.task_manager.update_task(
                    task_id,
                    progress=20 + int(prog * 0.6),  # 20-80%
                    message=msg
                )
            )
            
            # 5. è·å–å›¾è°±ä¿¡æ¯
            self.task_manager.update_task(
                task_id,
                progress=90,
                message="è·å–å›¾è°±ä¿¡æ¯..."
            )
            
            graph_info = self._get_graph_info(graph_id)
            
            # å®Œæˆ
            self.task_manager.complete_task(task_id, {
                "graph_id": graph_id,
                "graph_info": graph_info.to_dict(),
                "chunks_processed": total_chunks,
                "backend": "falkordb",
            })
            
        except Exception as e:
            import traceback
            error_msg = f"{str(e)}\n{traceback.format_exc()}"
            logger.error(f"FalkorDB å›¾è°±æ„å»ºå¤±è´¥: {error_msg}")
            self.task_manager.fail_task(task_id, error_msg)
    
    def create_graph(self, name: str) -> str:
        """åˆ›å»ºå›¾è°±"""
        graph_id = f"mirofish_{uuid.uuid4().hex[:16]}"
        
        # å­˜å‚¨å›¾è°±å…ƒæ•°æ®
        self._graphs[graph_id] = {
            "name": name,
            "created_at": datetime.now().isoformat(),
            "ontology": None,
        }
        
        # åˆ›å»ºç´¢å¼•ä»¥æé«˜æŸ¥è¯¢æ€§èƒ½
        try:
            graph = self.falkordb.get_graph(graph_id)
            # åˆ›å»ºèŠ‚ç‚¹ç´¢å¼•
            graph.query("CREATE INDEX FOR (n:Entity) ON (n.uuid)")
            graph.query("CREATE INDEX FOR (n:Entity) ON (n.name)")
        except Exception as e:
            logger.warning(f"åˆ›å»ºç´¢å¼•æ—¶å‡ºé”™ï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰: {e}")
        
        logger.info(f"FalkorDB å›¾è°±å·²åˆ›å»º: {graph_id} ({name})")
        return graph_id
    
    def set_ontology(self, graph_id: str, ontology: Dict[str, Any]):
        """è®¾ç½®å›¾è°±æœ¬ä½“"""
        if graph_id in self._graphs:
            self._graphs[graph_id]["ontology"] = ontology
        logger.info(f"FalkorDB æœ¬ä½“å·²è®¾ç½®: {graph_id}")
    
    def add_text_batches(
        self,
        graph_id: str,
        chunks: List[str],
        ontology: Dict[str, Any] = None,
        batch_size: int = 3,
        progress_callback: Optional[Callable] = None
    ) -> List[str]:
        """åˆ†æ‰¹æ·»åŠ æ–‡æœ¬åˆ°å›¾è°±"""
        entity_uuids = []
        total_chunks = len(chunks)
        
        for i, chunk in enumerate(chunks):
            chunk_num = i + 1
            
            if progress_callback:
                progress = (i + 1) / total_chunks
                progress_callback(
                    f"å¤„ç†ç¬¬ {chunk_num}/{total_chunks} ä¸ªæ–‡æœ¬å—...",
                    progress
                )
            
            try:
                # ä½¿ç”¨ LLM æå–å®ä½“å’Œå…³ç³»
                extraction = self.extractor.extract(chunk, ontology)
                
                # å­˜å‚¨å®ä½“
                for entity in extraction.get("entities", []):
                    entity_uuid = self._add_entity(graph_id, entity)
                    if entity_uuid:
                        entity_uuids.append(entity_uuid)
                
                # å­˜å‚¨å…³ç³»
                for relation in extraction.get("relations", []):
                    self._add_relation(graph_id, relation)
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(0.5)
                
            except Exception as e:
                logger.error(f"æ·»åŠ æ–‡æœ¬å— {chunk_num} å¤±è´¥: {e}")
                if progress_callback:
                    progress_callback(f"æ–‡æœ¬å— {chunk_num} å¤±è´¥: {str(e)}", 0)
        
        return entity_uuids
    
    def _add_entity(self, graph_id: str, entity: Dict[str, Any]) -> Optional[str]:
        """æ·»åŠ å®ä½“åˆ°å›¾è°±"""
        try:
            entity_uuid = uuid.uuid4().hex
            name = entity.get("name", "")
            entity_type = entity.get("type", "Entity")
            summary = entity.get("summary", "")
            attributes = json.dumps(entity.get("attributes", {}), ensure_ascii=False)
            
            # å…ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒåå®ä½“
            check_query = """
            MATCH (n:Entity {name: $name})
            RETURN n.uuid AS uuid
            """
            result = self.falkordb.execute_query(graph_id, check_query, {"name": name})
            
            if result.result_set:
                # å®ä½“å·²å­˜åœ¨ï¼Œè¿”å›ç°æœ‰ UUID
                return result.result_set[0][0]
            
            # åˆ›å»ºæ–°å®ä½“ - å°†ç±»å‹ä¿¡æ¯å­˜å‚¨åœ¨å±æ€§ä¸­è€Œä¸æ˜¯æ ‡ç­¾
            # å› ä¸º FalkorDB æ ‡ç­¾ä¸æ”¯æŒä¸­æ–‡
            safe_type = _sanitize_label(entity_type)
            labels = f"Entity:{safe_type}" if safe_type != "Entity" else "Entity"
            
            create_query = f"""
            CREATE (n:{labels} {{
                uuid: $uuid,
                name: $name,
                entity_type: $entity_type,
                summary: $summary,
                attributes: $attributes,
                created_at: $created_at
            }})
            RETURN n.uuid
            """
            
            self.falkordb.execute_query(graph_id, create_query, {
                "uuid": entity_uuid,
                "name": name,
                "entity_type": entity_type,  # ä¿ç•™åŸå§‹ç±»å‹å
                "summary": summary,
                "attributes": attributes,
                "created_at": datetime.now().isoformat()
            })
            
            logger.debug(f"å®ä½“å·²æ·»åŠ : {name} ({entity_type})")
            return entity_uuid
            
        except Exception as e:
            logger.error(f"æ·»åŠ å®ä½“å¤±è´¥: {e}")
            return None
    
    def _add_relation(self, graph_id: str, relation: Dict[str, Any]) -> bool:
        """æ·»åŠ å…³ç³»åˆ°å›¾è°±"""
        try:
            source_name = relation.get("source", "")
            target_name = relation.get("target", "")
            relation_type = relation.get("relation_type", "RELATED_TO").upper().replace(" ", "_")
            fact = relation.get("fact", "")
            
            if not source_name or not target_name:
                return False
            
            # å°†å…³ç³»ç±»å‹è½¬æ¢ä¸ºæœ‰æ•ˆçš„ Cypher æ ‡è¯†ç¬¦
            safe_relation_type = _sanitize_label(relation_type)
            
            # åˆ›å»ºå…³ç³»ï¼ˆå¦‚æœèŠ‚ç‚¹ä¸å­˜åœ¨åˆ™åŒæ—¶åˆ›å»ºï¼‰
            query = f"""
            MERGE (s:Entity {{name: $source_name}})
            ON CREATE SET s.uuid = $source_uuid, s.created_at = $created_at
            MERGE (t:Entity {{name: $target_name}})
            ON CREATE SET t.uuid = $target_uuid, t.created_at = $created_at
            MERGE (s)-[r:{safe_relation_type}]->(t)
            SET r.fact = $fact, r.uuid = $rel_uuid, r.relation_type = $original_type
            RETURN r.uuid
            """
            
            self.falkordb.execute_query(graph_id, query, {
                "source_name": source_name,
                "target_name": target_name,
                "source_uuid": uuid.uuid4().hex,
                "target_uuid": uuid.uuid4().hex,
                "rel_uuid": uuid.uuid4().hex,
                "fact": fact,
                "original_type": relation_type,  # ä¿ç•™åŸå§‹å…³ç³»ç±»å‹
                "created_at": datetime.now().isoformat()
            })
            
            logger.debug(f"å…³ç³»å·²æ·»åŠ : {source_name} --[{relation_type}]--> {target_name}")
            return True
            
        except Exception as e:
            logger.error(f"æ·»åŠ å…³ç³»å¤±è´¥: {e}")
            return False
    
    def _get_graph_info(self, graph_id: str) -> GraphInfo:
        """è·å–å›¾è°±ä¿¡æ¯"""
        try:
            # æŸ¥è¯¢èŠ‚ç‚¹æ•°é‡
            node_result = self.falkordb.execute_query(
                graph_id, 
                "MATCH (n:Entity) RETURN count(n) AS count"
            )
            node_count = node_result.result_set[0][0] if node_result.result_set else 0
            
            # æŸ¥è¯¢è¾¹æ•°é‡
            edge_result = self.falkordb.execute_query(
                graph_id,
                "MATCH ()-[r]->() RETURN count(r) AS count"
            )
            edge_count = edge_result.result_set[0][0] if edge_result.result_set else 0
            
            # æŸ¥è¯¢å®ä½“ç±»å‹
            labels_result = self.falkordb.execute_query(
                graph_id,
                "MATCH (n) RETURN DISTINCT labels(n) AS labels"
            )
            entity_types = set()
            for row in labels_result.result_set or []:
                for label in row[0]:
                    if label not in ["Entity", "Node"]:
                        entity_types.add(label)
            
            return GraphInfo(
                graph_id=graph_id,
                node_count=node_count,
                edge_count=edge_count,
                entity_types=list(entity_types)
            )
            
        except Exception as e:
            logger.error(f"è·å–å›¾è°±ä¿¡æ¯å¤±è´¥: {e}")
            return GraphInfo(graph_id=graph_id, node_count=0, edge_count=0, entity_types=[])
    
    def get_graph_data(self, graph_id: str) -> Dict[str, Any]:
        """è·å–å®Œæ•´å›¾è°±æ•°æ®"""
        try:
            # æŸ¥è¯¢æ‰€æœ‰èŠ‚ç‚¹
            nodes_result = self.falkordb.execute_query(
                graph_id,
                """
                MATCH (n:Entity)
                RETURN n.uuid AS uuid, n.name AS name, labels(n) AS labels, 
                       n.summary AS summary, n.attributes AS attributes
                """
            )
            
            nodes = []
            for row in nodes_result.result_set or []:
                nodes.append({
                    "uuid": row[0],
                    "name": row[1],
                    "labels": row[2],
                    "summary": row[3] or "",
                    "attributes": json.loads(row[4]) if row[4] else {}
                })
            
            # æŸ¥è¯¢æ‰€æœ‰è¾¹
            edges_result = self.falkordb.execute_query(
                graph_id,
                """
                MATCH (s:Entity)-[r]->(t:Entity)
                RETURN r.uuid AS uuid, type(r) AS type, r.fact AS fact,
                       s.uuid AS source_uuid, t.uuid AS target_uuid,
                       s.name AS source_name, t.name AS target_name
                """
            )
            
            edges = []
            for row in edges_result.result_set or []:
                edges.append({
                    "uuid": row[0],
                    "name": row[1],  # å‰ç«¯æœŸæœ› name å­—æ®µä½œä¸ºå…³ç³»ç±»å‹
                    "type": row[1],
                    "fact": row[2] or "",
                    "source_node_uuid": row[3],  # ä¸å‰ç«¯æœŸæœ›çš„å­—æ®µåä¸€è‡´
                    "target_node_uuid": row[4],  # ä¸å‰ç«¯æœŸæœ›çš„å­—æ®µåä¸€è‡´
                    "source_name": row[5],
                    "target_name": row[6],
                })
            
            return {
                "graph_id": graph_id,
                "nodes": nodes,
                "edges": edges,
                "node_count": len(nodes),
                "edge_count": len(edges),
                "backend": "falkordb",
            }
            
        except Exception as e:
            logger.error(f"è·å–å›¾è°±æ•°æ®å¤±è´¥: {e}")
            return {
                "graph_id": graph_id,
                "nodes": [],
                "edges": [],
                "node_count": 0,
                "edge_count": 0,
                "backend": "falkordb",
            }
    
    def delete_graph(self, graph_id: str):
        """åˆ é™¤å›¾è°±"""
        try:
            if graph_id in self._graphs:
                del self._graphs[graph_id]
            # FalkorDB åˆ é™¤æ‰€æœ‰èŠ‚ç‚¹å’Œå…³ç³»
            self.falkordb.execute_query(graph_id, "MATCH (n) DETACH DELETE n")
            logger.info(f"FalkorDB å›¾è°±å·²åˆ é™¤: {graph_id}")
        except Exception as e:
            logger.error(f"åˆ é™¤å›¾è°±å¤±è´¥: {e}")


class GraphitiEntityReader:
    """
    FalkorDB ç‰ˆå®ä½“è¯»å–æœåŠ¡
    æä¾›ä¸ ZepEntityReader å…¼å®¹çš„æ¥å£
    """
    
    def __init__(self, api_key: Optional[str] = None):
        """åˆå§‹åŒ–"""
        self._falkordb = None
    
    @property
    def falkordb(self) -> FalkorDBClient:
        """å»¶è¿Ÿåˆå§‹åŒ– FalkorDB å®¢æˆ·ç«¯"""
        if self._falkordb is None:
            self._falkordb = _get_falkordb_client()
        return self._falkordb
    
    def get_all_nodes(self, graph_id: str) -> List[Dict[str, Any]]:
        """è·å–å›¾è°±çš„æ‰€æœ‰èŠ‚ç‚¹"""
        logger.info(f"è·å–å›¾è°± {graph_id} çš„æ‰€æœ‰èŠ‚ç‚¹...")
        
        try:
            result = self.falkordb.execute_query(
                graph_id,
                """
                MATCH (n:Entity)
                RETURN n.uuid AS uuid, n.name AS name, labels(n) AS labels,
                       n.summary AS summary, n.attributes AS attributes
                """
            )
            
            nodes_data = []
            for row in result.result_set or []:
                nodes_data.append({
                    "uuid": row[0] or "",
                    "name": row[1] or "",
                    "labels": row[2] or [],
                    "summary": row[3] or "",
                    "attributes": json.loads(row[4]) if row[4] else {},
                })
            
            logger.info(f"å…±è·å– {len(nodes_data)} ä¸ªèŠ‚ç‚¹")
            return nodes_data
            
        except Exception as e:
            logger.error(f"è·å–èŠ‚ç‚¹å¤±è´¥: {e}")
            return []
    
    def get_all_edges(self, graph_id: str) -> List[Dict[str, Any]]:
        """è·å–å›¾è°±çš„æ‰€æœ‰è¾¹"""
        logger.info(f"è·å–å›¾è°± {graph_id} çš„æ‰€æœ‰è¾¹...")
        
        try:
            result = self.falkordb.execute_query(
                graph_id,
                """
                MATCH (s:Entity)-[r]->(t:Entity)
                RETURN r.uuid AS uuid, type(r) AS name, r.fact AS fact,
                       s.uuid AS source_node_uuid, t.uuid AS target_node_uuid
                """
            )
            
            edges_data = []
            for row in result.result_set or []:
                edges_data.append({
                    "uuid": row[0] or "",
                    "name": row[1] or "",
                    "fact": row[2] or "",
                    "source_node_uuid": row[3],
                    "target_node_uuid": row[4],
                    "attributes": {},
                })
            
            logger.info(f"å…±è·å– {len(edges_data)} æ¡è¾¹")
            return edges_data
            
        except Exception as e:
            logger.error(f"è·å–è¾¹å¤±è´¥: {e}")
            return []
    
    def get_node_edges(self, node_uuid: str) -> List[Any]:
        """è·å–æŒ‡å®šèŠ‚ç‚¹çš„æ‰€æœ‰ç›¸å…³è¾¹"""
        # éœ€è¦éå†æ‰€æœ‰å›¾è°±æŸ¥æ‰¾ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
        return []
    
    def filter_defined_entities(
        self, 
        graph_id: str,
        defined_entity_types: Optional[List[str]] = None,
        enrich_with_edges: bool = True
    ) -> FilteredEntities:
        """ç­›é€‰å‡ºç¬¦åˆé¢„å®šä¹‰å®ä½“ç±»å‹çš„èŠ‚ç‚¹"""
        logger.info(f"å¼€å§‹ç­›é€‰å›¾è°± {graph_id} çš„å®ä½“...")
        
        # è·å–æ‰€æœ‰èŠ‚ç‚¹
        all_nodes = self.get_all_nodes(graph_id)
        total_count = len(all_nodes)
        
        # è·å–æ‰€æœ‰è¾¹
        all_edges = self.get_all_edges(graph_id) if enrich_with_edges else []
        
        # æ„å»ºèŠ‚ç‚¹ UUID åˆ°èŠ‚ç‚¹æ•°æ®çš„æ˜ å°„
        node_map = {n["uuid"]: n for n in all_nodes}
        
        # ç­›é€‰ç¬¦åˆæ¡ä»¶çš„å®ä½“
        filtered_entities = []
        entity_types_found = set()
        
        for node in all_nodes:
            labels = node.get("labels", [])
            
            # ç­›é€‰é€»è¾‘ï¼šLabels å¿…é¡»åŒ…å«é™¤ "Entity" å’Œ "Node" ä¹‹å¤–çš„æ ‡ç­¾
            custom_labels = [l for l in labels if l not in ["Entity", "Node"]]
            
            if not custom_labels:
                continue
            
            # å¦‚æœæŒ‡å®šäº†é¢„å®šä¹‰ç±»å‹ï¼Œæ£€æŸ¥æ˜¯å¦åŒ¹é…
            if defined_entity_types:
                matching_labels = [l for l in custom_labels if l in defined_entity_types]
                if not matching_labels:
                    continue
                entity_type = matching_labels[0]
            else:
                entity_type = custom_labels[0]
            
            entity_types_found.add(entity_type)
            
            # åˆ›å»ºå®ä½“èŠ‚ç‚¹å¯¹è±¡
            entity = EntityNode(
                uuid=node["uuid"],
                name=node["name"],
                labels=labels,
                summary=node["summary"],
                attributes=node["attributes"],
            )
            
            # è·å–ç›¸å…³è¾¹å’ŒèŠ‚ç‚¹
            if enrich_with_edges:
                related_edges = []
                related_node_uuids = set()
                
                for edge in all_edges:
                    if edge["source_node_uuid"] == node["uuid"]:
                        related_edges.append({
                            "direction": "outgoing",
                            "edge_name": edge["name"],
                            "fact": edge["fact"],
                            "target_node_uuid": edge["target_node_uuid"],
                        })
                        related_node_uuids.add(edge["target_node_uuid"])
                    elif edge["target_node_uuid"] == node["uuid"]:
                        related_edges.append({
                            "direction": "incoming",
                            "edge_name": edge["name"],
                            "fact": edge["fact"],
                            "source_node_uuid": edge["source_node_uuid"],
                        })
                        related_node_uuids.add(edge["source_node_uuid"])
                
                entity.related_edges = related_edges
                
                # è·å–å…³è”èŠ‚ç‚¹çš„åŸºæœ¬ä¿¡æ¯
                related_nodes = []
                for related_uuid in related_node_uuids:
                    if related_uuid in node_map:
                        related_node = node_map[related_uuid]
                        related_nodes.append({
                            "uuid": related_node["uuid"],
                            "name": related_node["name"],
                            "labels": related_node["labels"],
                            "summary": related_node.get("summary", ""),
                        })
                
                entity.related_nodes = related_nodes
            
            filtered_entities.append(entity)
        
        logger.info(f"ç­›é€‰å®Œæˆ: æ€»èŠ‚ç‚¹ {total_count}, ç¬¦åˆæ¡ä»¶ {len(filtered_entities)}, "
                   f"å®ä½“ç±»å‹: {entity_types_found}")
        
        return FilteredEntities(
            entities=filtered_entities,
            entity_types=entity_types_found,
            total_count=total_count,
            filtered_count=len(filtered_entities),
        )
    
    def get_entity_with_context(
        self, 
        graph_id: str, 
        entity_uuid: str
    ) -> Optional[EntityNode]:
        """è·å–å•ä¸ªå®ä½“åŠå…¶å®Œæ•´ä¸Šä¸‹æ–‡"""
        try:
            # æŸ¥è¯¢å®ä½“
            result = self.falkordb.execute_query(
                graph_id,
                """
                MATCH (n:Entity {uuid: $uuid})
                RETURN n.uuid AS uuid, n.name AS name, labels(n) AS labels,
                       n.summary AS summary, n.attributes AS attributes
                """,
                {"uuid": entity_uuid}
            )
            
            if not result.result_set:
                return None
            
            row = result.result_set[0]
            entity = EntityNode(
                uuid=row[0],
                name=row[1],
                labels=row[2],
                summary=row[3] or "",
                attributes=json.loads(row[4]) if row[4] else {},
            )
            
            # æŸ¥è¯¢ç›¸å…³è¾¹
            edges_result = self.falkordb.execute_query(
                graph_id,
                """
                MATCH (n:Entity {uuid: $uuid})-[r]-(m:Entity)
                RETURN type(r) AS type, r.fact AS fact, 
                       m.uuid AS other_uuid, m.name AS other_name,
                       CASE WHEN startNode(r) = n THEN 'outgoing' ELSE 'incoming' END AS direction
                """,
                {"uuid": entity_uuid}
            )
            
            related_edges = []
            related_nodes = []
            seen_uuids = set()
            
            for edge_row in edges_result.result_set or []:
                related_edges.append({
                    "edge_name": edge_row[0],
                    "fact": edge_row[1] or "",
                    "other_uuid": edge_row[2],
                    "direction": edge_row[4],
                })
                
                if edge_row[2] not in seen_uuids:
                    seen_uuids.add(edge_row[2])
                    related_nodes.append({
                        "uuid": edge_row[2],
                        "name": edge_row[3],
                    })
            
            entity.related_edges = related_edges
            entity.related_nodes = related_nodes
            
            return entity
            
        except Exception as e:
            logger.error(f"è·å–å®ä½“ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return None
    
    def get_entities_by_type(
        self, 
        graph_id: str, 
        entity_type: str,
        enrich_with_edges: bool = True
    ) -> List[EntityNode]:
        """è·å–æŒ‡å®šç±»å‹çš„æ‰€æœ‰å®ä½“"""
        try:
            result = self.falkordb.execute_query(
                graph_id,
                f"""
                MATCH (n:{entity_type})
                RETURN n.uuid AS uuid, n.name AS name, labels(n) AS labels,
                       n.summary AS summary, n.attributes AS attributes
                """
            )
            
            entities = []
            for row in result.result_set or []:
                entity = EntityNode(
                    uuid=row[0],
                    name=row[1],
                    labels=row[2],
                    summary=row[3] or "",
                    attributes=json.loads(row[4]) if row[4] else {},
                )
                entities.append(entity)
            
            return entities
            
        except Exception as e:
            logger.error(f"è·å–ç±»å‹å®ä½“å¤±è´¥: {e}")
            return []


# å·¥å‚å‡½æ•°ï¼šæ ¹æ®é…ç½®è¿”å›åˆé€‚çš„æœåŠ¡å®ä¾‹
def get_graph_builder_service(api_key: Optional[str] = None):
    """
    è·å–å›¾è°±æ„å»ºæœåŠ¡
    æ ¹æ® USE_GRAPHITI é…ç½®è¿”å› FalkorDB æˆ– Zep å®ç°
    """
    if Config.USE_GRAPHITI:
        logger.info("ä½¿ç”¨ FalkorDB è‡ªæ‰˜ç®¡å›¾è°±æœåŠ¡")
        return GraphitiGraphBuilder(api_key)
    else:
        from .graph_builder import GraphBuilderService
        logger.info("ä½¿ç”¨ Zep Cloud å›¾è°±æœåŠ¡")
        return GraphBuilderService(api_key)


def get_entity_reader_service(api_key: Optional[str] = None):
    """
    è·å–å®ä½“è¯»å–æœåŠ¡
    æ ¹æ® USE_GRAPHITI é…ç½®è¿”å› FalkorDB æˆ– Zep å®ç°
    """
    if Config.USE_GRAPHITI:
        logger.info("ä½¿ç”¨ FalkorDB è‡ªæ‰˜ç®¡å®ä½“è¯»å–æœåŠ¡")
        return GraphitiEntityReader(api_key)
    else:
        from .zep_entity_reader import ZepEntityReader
        logger.info("ä½¿ç”¨ Zep Cloud å®ä½“è¯»å–æœåŠ¡")
        return ZepEntityReader(api_key)


# ==================== GraphitiToolsService ====================
# æä¾›ä¸ ZepToolsService å…¼å®¹çš„æ¥å£ï¼Œä½¿ report_agent å¯ä»¥ä½¿ç”¨ FalkorDB


@dataclass
class GraphitiSearchResult:
    """æœç´¢ç»“æœï¼ˆä¸ zep_tools.SearchResult å…¼å®¹ï¼‰"""
    facts: List[str]
    edges: List[Dict[str, Any]]
    nodes: List[Dict[str, Any]]
    query: str
    total_count: int
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "facts": self.facts,
            "edges": self.edges,
            "nodes": self.nodes,
            "query": self.query,
            "total_count": self.total_count
        }
    
    def to_text(self) -> str:
        """è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼ï¼Œä¾›LLMç†è§£"""
        text_parts = [f"æœç´¢æŸ¥è¯¢: {self.query}", f"æ‰¾åˆ° {self.total_count} æ¡ç›¸å…³ä¿¡æ¯"]
        
        if self.facts:
            text_parts.append("\n### ç›¸å…³äº‹å®:")
            for i, fact in enumerate(self.facts, 1):
                text_parts.append(f"{i}. {fact}")
        
        return "\n".join(text_parts)


@dataclass
class GraphitiInsightResult:
    """æ·±åº¦æ´å¯Ÿç»“æœï¼ˆä¸ zep_tools.InsightForgeResult å…¼å®¹ï¼‰"""
    query: str
    simulation_requirement: str
    sub_queries: List[str]
    semantic_facts: List[str] = field(default_factory=list)
    entity_insights: List[Dict[str, Any]] = field(default_factory=list)
    relationship_chains: List[str] = field(default_factory=list)
    total_facts: int = 0
    total_entities: int = 0
    total_relationships: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "query": self.query,
            "simulation_requirement": self.simulation_requirement,
            "sub_queries": self.sub_queries,
            "semantic_facts": self.semantic_facts,
            "entity_insights": self.entity_insights,
            "relationship_chains": self.relationship_chains,
            "total_facts": self.total_facts,
            "total_entities": self.total_entities,
            "total_relationships": self.total_relationships
        }
    
    def to_text(self) -> str:
        """è½¬æ¢ä¸ºè¯¦ç»†çš„æ–‡æœ¬æ ¼å¼"""
        text_parts = [
            f"## æ·±åº¦åˆ†æç»“æœ",
            f"åˆ†æé—®é¢˜: {self.query}",
            f"é¢„æµ‹åœºæ™¯: {self.simulation_requirement}",
            f"\n### æ•°æ®ç»Ÿè®¡",
            f"- ç›¸å…³äº‹å®: {self.total_facts}æ¡",
            f"- æ¶‰åŠå®ä½“: {self.total_entities}ä¸ª",
            f"- å…³ç³»é“¾: {self.total_relationships}æ¡"
        ]
        
        if self.sub_queries:
            text_parts.append(f"\n### åˆ†æçš„å­é—®é¢˜")
            for i, sq in enumerate(self.sub_queries, 1):
                text_parts.append(f"{i}. {sq}")
        
        if self.semantic_facts:
            text_parts.append(f"\n### ã€å…³é”®äº‹å®ã€‘")
            for i, fact in enumerate(self.semantic_facts, 1):
                text_parts.append(f'{i}. "{fact}"')
        
        if self.entity_insights:
            text_parts.append(f"\n### ã€æ ¸å¿ƒå®ä½“ã€‘")
            for entity in self.entity_insights:
                text_parts.append(f"- **{entity.get('name', 'æœªçŸ¥')}** ({entity.get('type', 'å®ä½“')})")
                if entity.get('summary'):
                    text_parts.append(f'  æ‘˜è¦: "{entity.get("summary")}"')
        
        if self.relationship_chains:
            text_parts.append(f"\n### ã€å…³ç³»é“¾ã€‘")
            for chain in self.relationship_chains:
                text_parts.append(f"- {chain}")
        
        return "\n".join(text_parts)


@dataclass
class GraphitiPanoramaResult:
    """å¹¿åº¦æœç´¢ç»“æœï¼ˆä¸ zep_tools.PanoramaResult å…¼å®¹ï¼‰"""
    query: str
    all_nodes: List[Dict[str, Any]] = field(default_factory=list)
    all_edges: List[Dict[str, Any]] = field(default_factory=list)
    active_facts: List[str] = field(default_factory=list)
    historical_facts: List[str] = field(default_factory=list)
    total_nodes: int = 0
    total_edges: int = 0
    active_count: int = 0
    historical_count: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "query": self.query,
            "all_nodes": self.all_nodes,
            "all_edges": self.all_edges,
            "active_facts": self.active_facts,
            "historical_facts": self.historical_facts,
            "total_nodes": self.total_nodes,
            "total_edges": self.total_edges,
            "active_count": self.active_count,
            "historical_count": self.historical_count
        }
    
    def to_text(self) -> str:
        """è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼"""
        text_parts = [
            f"## å¹¿åº¦æœç´¢ç»“æœ",
            f"æŸ¥è¯¢: {self.query}",
            f"\n### ç»Ÿè®¡ä¿¡æ¯",
            f"- æ€»èŠ‚ç‚¹æ•°: {self.total_nodes}",
            f"- æ€»è¾¹æ•°: {self.total_edges}",
            f"- å½“å‰æœ‰æ•ˆäº‹å®: {self.active_count}æ¡"
        ]
        
        if self.active_facts:
            text_parts.append(f"\n### ã€å½“å‰æœ‰æ•ˆäº‹å®ã€‘")
            for i, fact in enumerate(self.active_facts, 1):
                text_parts.append(f'{i}. "{fact}"')
        
        if self.all_nodes:
            text_parts.append(f"\n### ã€æ¶‰åŠå®ä½“ã€‘")
            for node in self.all_nodes[:20]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                name = node.get("name", "æœªçŸ¥")
                labels = node.get("labels", [])
                entity_type = next((l for l in labels if l not in ["Entity", "Node"]), "å®ä½“")
                text_parts.append(f"- **{name}** ({entity_type})")
        
        return "\n".join(text_parts)


@dataclass
class GraphitiInterviewResult:
    """é‡‡è®¿ç»“æœï¼ˆä¸ zep_tools.InterviewResult å…¼å®¹ï¼‰"""
    interview_topic: str
    interview_questions: List[str] = field(default_factory=list)
    selected_agents: List[Dict[str, Any]] = field(default_factory=list)
    interviews: List[Dict[str, Any]] = field(default_factory=list)
    selection_reasoning: str = ""
    summary: str = ""
    total_agents: int = 0
    interviewed_count: int = 0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "interview_topic": self.interview_topic,
            "interview_questions": self.interview_questions,
            "selected_agents": self.selected_agents,
            "interviews": self.interviews,
            "selection_reasoning": self.selection_reasoning,
            "summary": self.summary,
            "total_agents": self.total_agents,
            "interviewed_count": self.interviewed_count
        }
    
    def to_text(self) -> str:
        """è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼"""
        text_parts = [
            f"## ğŸ¤ Agenté‡‡è®¿æŠ¥å‘Š",
            f"**é‡‡è®¿ä¸»é¢˜:** {self.interview_topic}",
            f"**é‡‡è®¿äººæ•°:** {self.interviewed_count} ä½Agent"
        ]
        
        if self.selection_reasoning:
            text_parts.append(f"\n### é‡‡è®¿å¯¹è±¡é€‰æ‹©ç†ç”±")
            text_parts.append(self.selection_reasoning)
        
        if self.interviews:
            text_parts.append(f"\n### é‡‡è®¿å®å½•")
            for i, interview in enumerate(self.interviews, 1):
                agent_name = interview.get("agent_name", f"Agent_{i}")
                agent_role = interview.get("agent_role", "æœªçŸ¥è§’è‰²")
                response = interview.get("response", "")
                text_parts.append(f"\n#### {agent_name} ({agent_role})")
                text_parts.append(f"{response}")
        
        if self.summary:
            text_parts.append(f"\n### é‡‡è®¿æ‘˜è¦")
            text_parts.append(self.summary)
        
        return "\n".join(text_parts)


class GraphitiToolsService:
    """
    FalkorDB æ£€ç´¢å·¥å…·æœåŠ¡
    
    æä¾›ä¸ ZepToolsService å…¼å®¹çš„æ¥å£ï¼Œä½¿ report_agent å¯ä»¥åœ¨ FalkorDB åç«¯ä¸‹æ­£å¸¸å·¥ä½œã€‚
    """
    
    def __init__(self, api_key: Optional[str] = None, llm_client = None):
        self._falkordb = None
        self._llm_client = llm_client
        logger.info("GraphitiToolsService åˆå§‹åŒ–å®Œæˆï¼ˆä½¿ç”¨ FalkorDBï¼‰")
    
    @property
    def falkordb(self) -> FalkorDBClient:
        """å»¶è¿Ÿåˆå§‹åŒ– FalkorDB å®¢æˆ·ç«¯"""
        if self._falkordb is None:
            self._falkordb = _get_falkordb_client()
        return self._falkordb
    
    @property
    def llm(self):
        """å»¶è¿Ÿåˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        if self._llm_client is None:
            from ..utils.llm_client import LLMClient
            self._llm_client = LLMClient()
        return self._llm_client
    
    def _get_all_nodes(self, graph_id: str) -> List[Dict[str, Any]]:
        """è·å–å›¾è°±çš„æ‰€æœ‰èŠ‚ç‚¹"""
        try:
            result = self.falkordb.execute_query(
                graph_id,
                """
                MATCH (n:Entity)
                RETURN n.uuid AS uuid, n.name AS name, labels(n) AS labels,
                       n.summary AS summary, n.attributes AS attributes, n.entity_type AS entity_type
                """
            )
            
            nodes = []
            for row in result.result_set or []:
                nodes.append({
                    "uuid": row[0] or "",
                    "name": row[1] or "",
                    "labels": row[2] or [],
                    "summary": row[3] or "",
                    "attributes": json.loads(row[4]) if row[4] else {},
                    "entity_type": row[5] or ""
                })
            return nodes
        except Exception as e:
            logger.error(f"è·å–èŠ‚ç‚¹å¤±è´¥: {e}")
            return []
    
    def _get_all_edges(self, graph_id: str) -> List[Dict[str, Any]]:
        """è·å–å›¾è°±çš„æ‰€æœ‰è¾¹"""
        try:
            result = self.falkordb.execute_query(
                graph_id,
                """
                MATCH (s:Entity)-[r]->(t:Entity)
                RETURN r.uuid AS uuid, type(r) AS name, r.fact AS fact,
                       s.uuid AS source_node_uuid, t.uuid AS target_node_uuid,
                       s.name AS source_name, t.name AS target_name
                """
            )
            
            edges = []
            for row in result.result_set or []:
                edges.append({
                    "uuid": row[0] or "",
                    "name": row[1] or "",
                    "fact": row[2] or "",
                    "source_node_uuid": row[3] or "",
                    "target_node_uuid": row[4] or "",
                    "source_name": row[5] or "",
                    "target_name": row[6] or ""
                })
            return edges
        except Exception as e:
            logger.error(f"è·å–è¾¹å¤±è´¥: {e}")
            return []
    
    def _keyword_search(self, items: List[Dict], query: str, key_fields: List[str], limit: int = 10) -> List[Dict]:
        """ç®€å•çš„å…³é”®è¯æœç´¢"""
        query_lower = query.lower()
        keywords = [w.strip() for w in query_lower.replace(',', ' ').replace('ï¼Œ', ' ').split() if len(w.strip()) > 1]
        
        def score(item: Dict) -> int:
            s = 0
            for field in key_fields:
                text = str(item.get(field, "")).lower()
                if query_lower in text:
                    s += 100
                for kw in keywords:
                    if kw in text:
                        s += 10
            return s
        
        scored = [(score(item), item) for item in items]
        scored.sort(key=lambda x: x[0], reverse=True)
        return [item for s, item in scored[:limit] if s > 0]
    
    def get_graph_statistics(self, graph_id: str) -> Dict[str, Any]:
        """è·å–å›¾è°±ç»Ÿè®¡ä¿¡æ¯"""
        try:
            nodes = self._get_all_nodes(graph_id)
            edges = self._get_all_edges(graph_id)
            
            # ç»Ÿè®¡å®ä½“ç±»å‹
            entity_types = {}
            for node in nodes:
                for label in node.get("labels", []):
                    if label not in ["Entity", "Node"]:
                        entity_types[label] = entity_types.get(label, 0) + 1
            
            return {
                "total_nodes": len(nodes),
                "total_edges": len(edges),
                "entity_types": entity_types
            }
        except Exception as e:
            logger.error(f"è·å–å›¾è°±ç»Ÿè®¡å¤±è´¥: {e}")
            return {"total_nodes": 0, "total_edges": 0, "entity_types": {}}
    
    def get_simulation_context(
        self, 
        graph_id: str, 
        simulation_requirement: str, 
        limit: int = 20
    ) -> Dict[str, Any]:
        """è·å–æ¨¡æ‹Ÿä¸Šä¸‹æ–‡"""
        logger.info(f"è·å–æ¨¡æ‹Ÿä¸Šä¸‹æ–‡: {simulation_requirement[:50]}...")
        
        # è·å–æ‰€æœ‰æ•°æ®
        all_nodes = self._get_all_nodes(graph_id)
        all_edges = self._get_all_edges(graph_id)
        
        # æœç´¢ç›¸å…³äº‹å®
        related_edges = self._keyword_search(all_edges, simulation_requirement, ["fact", "name"], limit)
        facts = [e.get("fact", "") for e in related_edges if e.get("fact")]
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = self.get_graph_statistics(graph_id)
        
        # ç­›é€‰æœ‰ç±»å‹çš„å®ä½“
        entities = []
        for node in all_nodes:
            custom_labels = [l for l in node.get("labels", []) if l not in ["Entity", "Node"]]
            if custom_labels:
                entities.append({
                    "name": node.get("name", ""),
                    "type": custom_labels[0],
                    "summary": node.get("summary", "")
                })
        
        return {
            "simulation_requirement": simulation_requirement,
            "related_facts": facts,
            "graph_statistics": stats,
            "entities": entities[:limit],
            "total_entities": len(entities)
        }
    
    def quick_search(self, graph_id: str, query: str, limit: int = 10) -> GraphitiSearchResult:
        """ç®€å•æœç´¢"""
        logger.info(f"å¿«é€Ÿæœç´¢: {query[:30]}...")
        
        all_edges = self._get_all_edges(graph_id)
        all_nodes = self._get_all_nodes(graph_id)
        
        # æœç´¢è¾¹
        matched_edges = self._keyword_search(all_edges, query, ["fact", "name", "source_name", "target_name"], limit)
        facts = [e.get("fact", "") for e in matched_edges if e.get("fact")]
        
        # æœç´¢èŠ‚ç‚¹
        matched_nodes = self._keyword_search(all_nodes, query, ["name", "summary"], limit)
        for node in matched_nodes:
            if node.get("summary"):
                facts.append(f"[{node.get('name')}]: {node.get('summary')}")
        
        return GraphitiSearchResult(
            facts=facts,
            edges=matched_edges,
            nodes=matched_nodes,
            query=query,
            total_count=len(facts)
        )
    
    def insight_forge(
        self,
        graph_id: str,
        query: str,
        simulation_requirement: str,
        report_context: str = "",
        max_sub_queries: int = 5
    ) -> GraphitiInsightResult:
        """æ·±åº¦æ´å¯Ÿæ£€ç´¢"""
        logger.info(f"æ·±åº¦æ´å¯Ÿæ£€ç´¢: {query[:50]}...")
        
        result = GraphitiInsightResult(
            query=query,
            simulation_requirement=simulation_requirement,
            sub_queries=[]
        )
        
        # è·å–æ‰€æœ‰æ•°æ®
        all_nodes = self._get_all_nodes(graph_id)
        all_edges = self._get_all_edges(graph_id)
        
        # ä½¿ç”¨ LLM ç”Ÿæˆå­é—®é¢˜ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        sub_queries = [query]  # è‡³å°‘åŒ…å«åŸå§‹æŸ¥è¯¢
        result.sub_queries = sub_queries
        
        # æ”¶é›†æ‰€æœ‰ç›¸å…³äº‹å®
        all_facts = []
        seen_facts = set()
        
        for sub_query in sub_queries:
            matched_edges = self._keyword_search(all_edges, sub_query, ["fact", "name"], 20)
            for edge in matched_edges:
                fact = edge.get("fact", "")
                if fact and fact not in seen_facts:
                    all_facts.append(fact)
                    seen_facts.add(fact)
        
        result.semantic_facts = all_facts
        result.total_facts = len(all_facts)
        
        # æ”¶é›†ç›¸å…³å®ä½“
        entity_insights = []
        for node in all_nodes:
            custom_labels = [l for l in node.get("labels", []) if l not in ["Entity", "Node"]]
            if custom_labels:
                entity_type = custom_labels[0]
            else:
                entity_type = node.get("entity_type", "å®ä½“")
            
            entity_insights.append({
                "uuid": node.get("uuid", ""),
                "name": node.get("name", ""),
                "type": entity_type,
                "summary": node.get("summary", ""),
                "related_facts": []
            })
        
        result.entity_insights = entity_insights[:30]  # é™åˆ¶æ•°é‡
        result.total_entities = len(entity_insights)
        
        # æ„å»ºå…³ç³»é“¾
        relationship_chains = []
        for edge in all_edges[:50]:  # é™åˆ¶æ•°é‡
            source = edge.get("source_name", edge.get("source_node_uuid", "")[:8])
            target = edge.get("target_name", edge.get("target_node_uuid", "")[:8])
            relation = edge.get("name", "RELATED")
            chain = f"{source} --[{relation}]--> {target}"
            relationship_chains.append(chain)
        
        result.relationship_chains = relationship_chains
        result.total_relationships = len(relationship_chains)
        
        logger.info(f"æ·±åº¦æ´å¯Ÿå®Œæˆ: {result.total_facts}æ¡äº‹å®, {result.total_entities}ä¸ªå®ä½“")
        return result
    
    def panorama_search(
        self,
        graph_id: str,
        query: str,
        include_expired: bool = True
    ) -> GraphitiPanoramaResult:
        """å¹¿åº¦æœç´¢"""
        logger.info(f"å¹¿åº¦æœç´¢: {query[:30]}...")
        
        all_nodes = self._get_all_nodes(graph_id)
        all_edges = self._get_all_edges(graph_id)
        
        # æ”¶é›†æ‰€æœ‰äº‹å®
        active_facts = [e.get("fact", "") for e in all_edges if e.get("fact")]
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        if query:
            matched_edges = self._keyword_search(all_edges, query, ["fact", "name"], len(all_edges))
            active_facts = [e.get("fact", "") for e in matched_edges if e.get("fact")]
        
        return GraphitiPanoramaResult(
            query=query,
            all_nodes=all_nodes,
            all_edges=all_edges,
            active_facts=active_facts,
            historical_facts=[],  # FalkorDB ä¸æ”¯æŒæ—¶é—´ç»´åº¦
            total_nodes=len(all_nodes),
            total_edges=len(all_edges),
            active_count=len(active_facts),
            historical_count=0
        )
    
    def interview_agents(
        self,
        simulation_id: str,
        interview_requirement: str,
        simulation_requirement: str,
        max_agents: int = 5
    ) -> GraphitiInterviewResult:
        """é‡‡è®¿ Agentï¼ˆç®€åŒ–å®ç°ï¼‰"""
        logger.info(f"é‡‡è®¿ Agent: {interview_requirement[:30]}...")
        
        # è¿™é‡Œç®€åŒ–å®ç°ï¼Œè¿”å›ç©ºç»“æœ
        # å®Œæ•´å®ç°éœ€è¦è°ƒç”¨ OASIS é‡‡è®¿ API
        return GraphitiInterviewResult(
            interview_topic=interview_requirement,
            interview_questions=[],
            selected_agents=[],
            interviews=[],
            selection_reasoning="FalkorDB æ¨¡å¼ä¸‹æš‚ä¸æ”¯æŒ Agent é‡‡è®¿åŠŸèƒ½",
            summary="",
            total_agents=0,
            interviewed_count=0
        )
    
    def get_entity_summary(self, graph_id: str, entity_name: str) -> Dict[str, Any]:
        """è·å–å®ä½“æ‘˜è¦"""
        try:
            result = self.falkordb.execute_query(
                graph_id,
                """
                MATCH (n:Entity {name: $name})
                OPTIONAL MATCH (n)-[r]-(m:Entity)
                RETURN n.uuid AS uuid, n.name AS name, labels(n) AS labels,
                       n.summary AS summary, collect(DISTINCT {type: type(r), other: m.name}) AS relations
                """,
                {"name": entity_name}
            )
            
            if not result.result_set:
                return {"error": f"æœªæ‰¾åˆ°å®ä½“: {entity_name}"}
            
            row = result.result_set[0]
            return {
                "uuid": row[0],
                "name": row[1],
                "labels": row[2],
                "summary": row[3],
                "relations": row[4] or []
            }
        except Exception as e:
            logger.error(f"è·å–å®ä½“æ‘˜è¦å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def get_entities_by_type(self, graph_id: str, entity_type: str) -> List[Dict[str, Any]]:
        """æŒ‰ç±»å‹è·å–å®ä½“"""
        try:
            # ä½¿ç”¨ _sanitize_label è½¬æ¢åçš„ç±»å‹æŸ¥è¯¢
            safe_type = _sanitize_label(entity_type)
            
            result = self.falkordb.execute_query(
                graph_id,
                f"""
                MATCH (n:{safe_type})
                RETURN n.uuid AS uuid, n.name AS name, labels(n) AS labels,
                       n.summary AS summary
                """
            )
            
            entities = []
            for row in result.result_set or []:
                entities.append({
                    "uuid": row[0],
                    "name": row[1],
                    "labels": row[2],
                    "summary": row[3] or ""
                })
            
            return entities
        except Exception as e:
            logger.error(f"è·å–ç±»å‹å®ä½“å¤±è´¥: {e}")
            return []


def get_tools_service(api_key: Optional[str] = None):
    """
    è·å–æ£€ç´¢å·¥å…·æœåŠ¡
    æ ¹æ® USE_GRAPHITI é…ç½®è¿”å› FalkorDB æˆ– Zep å®ç°
    """
    if Config.USE_GRAPHITI:
        logger.info("ä½¿ç”¨ FalkorDB è‡ªæ‰˜ç®¡æ£€ç´¢å·¥å…·æœåŠ¡")
        return GraphitiToolsService(api_key)
    else:
        from .zep_tools import ZepToolsService
        logger.info("ä½¿ç”¨ Zep Cloud æ£€ç´¢å·¥å…·æœåŠ¡")
        return ZepToolsService(api_key)
